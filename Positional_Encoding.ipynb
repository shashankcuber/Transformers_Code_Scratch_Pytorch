{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Embedding + PE -> Encoder \\\n",
        "PE (pos, 2i) = sin(pos/ 10000^(2i/d_model)) \\\n",
        "PE (pos, 2i+1) = sin(pos/ 10000^(2i/d_model)) \\\n",
        "Here pos is the position of the word in the sequence and i is the index of the dimension and d_model is the dimension of the model here 512. \\\n",
        "Why we have taken cosin and sin function due to periodicity a word can pay attention to word at farther distance much better due to periodicity. And it constraints the value between -1 and 1. \\\n",
        "During the attention suppose without sine and cosine the word at position i would not be able to attend to words farther away as their position keeps growing as compared to current word i but the sine and cosine helps to maintain the order range from -1 to 1 eliminating this limitation. \\\n",
        "Easy to extrapolate to longer sequences or sequence lengths which are not there in the training set."
      ],
      "metadata": {
        "id": "Qpk59tNj2V0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDIngstMHAxb"
      },
      "outputs": [],
      "source": [
        "# Let's walk through the code\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_sequence_length = 10\n",
        "d_model = 6 # dimension of the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "even_i = torch.arange(0, d_model, 2).float() # create values from 0-d_model skipping 2 so taking even positions\n",
        "even_i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioU4gIGh5Hs1",
        "outputId": "c570b4ea-ca5e-4208-fd0a-6d93f9c64e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "even_denominator = torch.pow(10000, even_i/d_model)\n",
        "even_denominator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqrUrnmi5V4d",
        "outputId": "ab68913c-fb29-4049-82b5-7aa52656a8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  1.0000,  21.5443, 464.1590])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same thing for odd dimensions\n",
        "odd_i = torch.arange(1, d_model, 2).float()\n",
        "odd_i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AqvRs-L5iJd",
        "outputId": "db0c1108-41af-4eb9-c185-dbb5f52c83c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 3., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "odd_denominator = torch.pow(10000, odd_i/d_model)\n",
        "odd_denominator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAGJ2k9W5tVB",
        "outputId": "082094f9-796d-4054-8039-d52a11c6e83a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   4.6416,  100.0000, 2154.4343])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now since i-1 is even so denominators becomes essentially the same?? For this first see how the positional encoding is reformulated."
      ],
      "metadata": {
        "id": "yliywLOl50z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "denominator = even_denominator"
      ],
      "metadata": {
        "id": "0ERnE7s16GUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position = torch.arange(max_sequence_length, dtype=torch.float).reshape(max_sequence_length, 1) # it determines the position of each word in the sequence\n",
        "position.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MRZrxRu6JLr",
        "outputId": "310503d6-5822-4cc8-c722-d90ba56fd037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "even_PE = torch.sin(position/ denominator)\n",
        "even_PE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_gZoQh56TLW",
        "outputId": "2c7541c1-2f2b-4cf7-e696-713c7c85aa9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000],\n",
              "        [ 0.8415,  0.0464,  0.0022],\n",
              "        [ 0.9093,  0.0927,  0.0043],\n",
              "        [ 0.1411,  0.1388,  0.0065],\n",
              "        [-0.7568,  0.1846,  0.0086],\n",
              "        [-0.9589,  0.2300,  0.0108],\n",
              "        [-0.2794,  0.2749,  0.0129],\n",
              "        [ 0.6570,  0.3192,  0.0151],\n",
              "        [ 0.9894,  0.3629,  0.0172],\n",
              "        [ 0.4121,  0.4057,  0.0194]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "odd_PE = torch.cos(position / denominator)\n",
        "odd_PE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI3zFjaX_xGB",
        "outputId": "6c4c4d8d-7e41-403e-cddc-363834485489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0000,  1.0000,  1.0000],\n",
              "        [ 0.5403,  0.9989,  1.0000],\n",
              "        [-0.4161,  0.9957,  1.0000],\n",
              "        [-0.9900,  0.9903,  1.0000],\n",
              "        [-0.6536,  0.9828,  1.0000],\n",
              "        [ 0.2837,  0.9732,  0.9999],\n",
              "        [ 0.9602,  0.9615,  0.9999],\n",
              "        [ 0.7539,  0.9477,  0.9999],\n",
              "        [-0.1455,  0.9318,  0.9999],\n",
              "        [-0.9111,  0.9140,  0.9998]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are all 10x3 because odd and even are separated in embedding which was of size 10x6"
      ],
      "metadata": {
        "id": "jV3kO6QUABU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we have to interleave the odd and even indices to get 10x6 dimension positional encodin\n",
        "stacked = torch.stack([even_PE, odd_PE], dim=-1)\n",
        "stacked.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFN4MpPn_7KT",
        "outputId": "288a65a5-4752-484f-dee3-39d00356dd02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now flatten it\n",
        "PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "PE.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HZnM5LwAYim",
        "outputId": "63f25d19-354f-462b-82ef-8a02b7575ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzbFbFOFAl8L",
        "outputId": "5977615f-4f79-4e0c-8aaa-30218a7202f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
              "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
              "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
              "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
              "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
              "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
              "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
              "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
              "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class Implementation\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_seq_length):\n",
        "    super().__init__()\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def forward(self):\n",
        "    even_i = torch.arange(0, self.d_model, 2).float()\n",
        "    odd_i = torch.arange(1, self.d_model, 2).float()\n",
        "\n",
        "    denominator = torch.pow(10000, even_i/self.d_model)\n",
        "    position = torch.arange(self.max_seq_length).reshape(self.max_seq_length, 1)\n",
        "\n",
        "    even_PE = torch.sin(position/denominator)\n",
        "    odd_PE = torch.cos(position/denominator)\n",
        "\n",
        "    stacked = torch.stack([even_PE, odd_PE], dim = 2)\n",
        "\n",
        "    PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "\n",
        "    return PE"
      ],
      "metadata": {
        "id": "RGEPaJiIAsAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding(d_model=6, max_seq_length=10)\n",
        "pe() # forward automaitcally get's called for class with nn.Module"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9AOgKNaB4yi",
        "outputId": "7cefc4a6-cc8c-4ddb-fc46-5d4921da18a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
              "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
              "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
              "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
              "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
              "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
              "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
              "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
              "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}